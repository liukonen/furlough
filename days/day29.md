# Day 29

I got sucked into working on my homepage.... it's been bugging me that it's not IE 11 compliant. Decided to work on some testing and JS, like custom buttons when its a website vs a release.
Modernizr has been a godsend when it comes to fixing these issues.

I was going to watch and learn a bunch on the Go language, but that can wait.
Instead, I downloaded and installed the Stanford Core NLP docker engine on my home server. [link](https://stanfordnlp.github.io/CoreNLP/other-languages.html)
I wanted to investigate this and potentially use it in my last hackathon at work. The concept was to extract keyword and sentiment analysis from trip-advisor reviews. Normalizing and gaining metrics on items that previously were hard to capture. I do want to play with this, as well as Apache Open NLP [link](https://opennlp.apache.org/) in the near future. The Azure cognitive services are absolutely great (the engine we used for the hackathon) however each call as an expense associated with it. For enterprise, if we can use an open source platform, while also keeping our code associated with it separate from the core engine, in theory, it's kosher. 

Languages are fun and all, however, some of the items from my workplace focus more on big data, power bi, and those tools.
excel is fun and all (power bi is a glorified live data version of excel from my recent training), but there is only so much you can do... Maybe watching some case studies and examples might inspire me, however, I feel I'd be more valuable if I can learn how to extract and generate data from un normalized sources then stats we already have.

Watched a video on merging ML based models in a MS chatbot. One thing I spotted was none of it is real time... and once the model is trained... its trained. Wondering how you could real time this [link to video](https://channel9.msdn.com/events/dotnetConf/NET-Conf-2019/B328?term=ml.net&lang-en=true)
